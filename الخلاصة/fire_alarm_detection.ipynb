{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a366e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from time import time\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "from io import BytesIO\n",
    "\n",
    "class ObjectDetection:\n",
    "\n",
    "    def __init__(self, capture_index):\n",
    "        self.capture_index = capture_index\n",
    "        self.device = 'cpu'\n",
    "        print(\"Using Device: \", self.device)\n",
    "        self.model = self.load_model()\n",
    "        self.CLASS_NAMES_DICT = self.model.model.names\n",
    "\n",
    "    def load_model(self):\n",
    "        model = YOLO(r\"C:\\Users\\mmmel\\Desktop\\AI_project\\best.pt\")\n",
    "        model.fuse()\n",
    "        return model\n",
    "\n",
    "    def predict(self, frame):\n",
    "        results = self.model(frame)\n",
    "        return results\n",
    "\n",
    "    def plot_bboxes(self, results, frame):\n",
    "        for result in results[0]:\n",
    "            class_id = result.boxes.cls.cpu().numpy().astype(int)\n",
    "            if class_id == 1:\n",
    "                confidence = result.boxes.conf.cpu().numpy()\n",
    "                label = f\"{self.CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n",
    "                box = result.boxes.xyxy.cpu().numpy().astype(int)\n",
    "\n",
    "                # Draw bounding box on the frame\n",
    "                cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 3)\n",
    "                cv2.putText(frame, label, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def display_image(self, frame):\n",
    "        _, img_encoded = cv2.imencode('.png', frame)\n",
    "        display(Image(data=img_encoded.tobytes()))\n",
    "\n",
    "    def __call__(self):\n",
    "        cap = cv2.VideoCapture(self.capture_index)\n",
    "        assert cap.isOpened()\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                start_time = time()\n",
    "                ret, frame = cap.read()\n",
    "                assert ret\n",
    "                results = self.predict(frame)\n",
    "                frame = self.plot_bboxes(results, frame)\n",
    "                end_time = time()\n",
    "                fps = 1/np.round(end_time - start_time, 2)\n",
    "                cv2.putText(frame, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "                # Display the frame using IPython.display\n",
    "                self.display_image(frame)\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            # Handle KeyboardInterrupt to stop the video stream gracefully\n",
    "            pass\n",
    "        finally:\n",
    "            cap.release()\n",
    "\n",
    "detector = ObjectDetection(capture_index=0)\n",
    "detector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56402651",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from time import time\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "from io import BytesIO\n",
    "import logging\n",
    "\n",
    "class ObjectDetection:\n",
    "\n",
    "    def __init__(self, capture_index=0, model_path=\"C:\\\\Users\\\\mmmel\\\\Desktop\\\\AI_project\\\\best.pt\", confidence_threshold=0.5):\n",
    "        self.capture_index = capture_index\n",
    "        self.device = 'cpu'\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        logging.info(\"Using Device: %s\", self.device)\n",
    "\n",
    "        # Load YOLO model\n",
    "        self.model = self.load_model(model_path)\n",
    "        self.CLASS_NAMES_DICT = self.model.model.names\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        try:\n",
    "            model = YOLO(model_path)\n",
    "            model.fuse()\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error loading the model: %s\", str(e))\n",
    "            raise\n",
    "\n",
    "    def predict(self, frame):\n",
    "        try:\n",
    "            results = self.model(frame)\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error during prediction: %s\", str(e))\n",
    "            raise\n",
    "\n",
    "    def plot_bboxes(self, results, frame):\n",
    "        for result in results[0]:\n",
    "            class_id = result.boxes.cls.cpu().numpy().astype(int)\n",
    "            confidence = result.boxes.conf.cpu().numpy()\n",
    "\n",
    "            # Adjust this condition based on your class index\n",
    "            if class_id == 1 and confidence > self.confidence_threshold:\n",
    "                label = f\"{self.CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n",
    "                box = result.boxes.xyxy.cpu().numpy().astype(int)\n",
    "\n",
    "                # Print bounding box coordinates for debugging\n",
    "                logging.debug(\"Bounding Box Coordinates: %s\", box)\n",
    "\n",
    "                # Draw bounding box on the frame\n",
    "                cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 3)\n",
    "                cv2.putText(frame, label, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def display_image(self, frame):\n",
    "        _, img_encoded = cv2.imencode('.png', frame)\n",
    "        display(Image(data=img_encoded.tobytes()))\n",
    "\n",
    "    def configure_camera(self, cap):\n",
    "        assert cap.isOpened()\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "    def capture_frames(self):\n",
    "        cap = cv2.VideoCapture(self.capture_index)\n",
    "        self.configure_camera(cap)\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                start_time = time()\n",
    "                ret, frame = cap.read()\n",
    "                assert ret\n",
    "                results = self.predict(frame)\n",
    "                frame = self.plot_bboxes(results, frame)\n",
    "                end_time = time()\n",
    "                fps = 1 / np.round(end_time - start_time, 2)\n",
    "                cv2.putText(frame, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "                # Display the frame using IPython.display\n",
    "                self.display_image(frame)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            # Handle KeyboardInterrupt to stop the video stream gracefully\n",
    "            pass\n",
    "        finally:\n",
    "            cap.release()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    detector = ObjectDetection(capture_index=0)\n",
    "    detector.capture_frames()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76796323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c76b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from time import time, sleep\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "import tkinter as tk\n",
    "from playsound import playsound\n",
    "\n",
    "class ObjectDetection:\n",
    "\n",
    "    def __init__(self, capture_index):\n",
    "        self.capture_index = capture_index\n",
    "        self.device = 'cpu'\n",
    "        print(\"Using Device: \", self.device)\n",
    "        self.model = self.load_model()\n",
    "        self.CLASS_NAMES_DICT = self.model.model.names\n",
    "\n",
    "        # GUI setup\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"Fire Detection Alarm\")\n",
    "        self.alarm_label = tk.Label(self.root, text=\"No Fire Detected\", font=(\"Helvetica\", 16))\n",
    "        self.alarm_label.pack()\n",
    "\n",
    "    def load_model(self):\n",
    "        model = YOLO(r\"C:\\Users\\mmmel\\Desktop\\AI_project\\best.pt\")\n",
    "        model.fuse()\n",
    "        return model\n",
    "\n",
    "    def predict(self, frame):\n",
    "        results = self.model(frame)\n",
    "        #print(results)\n",
    "        return results\n",
    "\n",
    "    def plot_bboxes(self, results):\n",
    "        fire_detected = False\n",
    "\n",
    "        for result in results[0]:\n",
    "            class_id = result.boxes.cls.cpu().numpy().astype(int)\n",
    "            if class_id == 0:\n",
    "                \n",
    "                fire_detected = True\n",
    "\n",
    "        # Update GUI label\n",
    "        if fire_detected:\n",
    "            self.alarm_label.config(text=\"Fire Detected!\", fg=\"red\")\n",
    "            # Play alarm sound\n",
    "            #playsound(r\"C:\\Users\\mmmel\\Desktop\\AI_project\\fire-alarm-9677.mp3\")  # Replace with the actual path to your alarm sound file\n",
    "        else:\n",
    "            self.alarm_label.config(text=\"No Fire Detected\", fg=\"green\")\n",
    "\n",
    "    def display_image(self, frame):\n",
    "        _, img_encoded = cv2.imencode('.png', frame)\n",
    "        display(Image(data=img_encoded.tobytes()))\n",
    "\n",
    "    def update_gui(self):\n",
    "        self.root.update()\n",
    "\n",
    "    def __call__(self):\n",
    "        cap = cv2.VideoCapture(self.capture_index)\n",
    "        assert cap.isOpened()\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                start_time = time()\n",
    "                ret, frame = cap.read()\n",
    "                assert ret\n",
    "                results = self.predict(frame)\n",
    "                self.plot_bboxes(results)\n",
    "                end_time = time()\n",
    "                fps = 1/np.round(end_time - start_time, 2)\n",
    "                cv2.putText(frame, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "                # Display the frame using IPython.display\n",
    "                self.display_image(frame)\n",
    "                self.update_gui()\n",
    "                sleep(0.01)  # Small delay to allow GUI to update\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"KeyboardInterrupt: Stopping the video stream and closing the GUI gracefully.\")\n",
    "        finally:\n",
    "            cap.release()\n",
    "            self.root.destroy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detector = ObjectDetection(capture_index=0)\n",
    "    detector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from time import time, sleep\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "import tkinter as tk\n",
    "from playsound import playsound\n",
    "\n",
    "class ObjectDetection:\n",
    "\n",
    "    def __init__(self, capture_index):\n",
    "        self.capture_index = capture_index\n",
    "        self.device = 'cpu'\n",
    "        print(\"Using Device: \", self.device)\n",
    "        self.model = self.load_model()\n",
    "        self.CLASS_NAMES_DICT = self.model.model.names\n",
    "\n",
    "        # GUI setup\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"Fire Detection Alarm\")\n",
    "        self.alarm_label = tk.Label(self.root, text=\"No Fire Detected\", font=(\"Helvetica\", 16))\n",
    "        self.alarm_label.pack()\n",
    "\n",
    "    def load_model(self):\n",
    "        model = YOLO(r\"C:\\Users\\mmmel\\Desktop\\AI_project\\best.pt\")\n",
    "        model.fuse()\n",
    "        return model\n",
    "\n",
    "    def predict(self, frame):\n",
    "        results = self.model(frame)\n",
    "        #print(results)\n",
    "        return results\n",
    "\n",
    "    def plot_bboxes(self, results):\n",
    "        fire_detected = False\n",
    "\n",
    "        for result in results[0]:\n",
    "            class_id = result.boxes.cls.cpu().numpy().astype(int)\n",
    "            if class_id == 0:\n",
    "                \n",
    "                fire_detected = True\n",
    "\n",
    "        # Update GUI label\n",
    "        if fire_detected:\n",
    "            self.alarm_label.config(text=\"Fire Detected!\", fg=\"red\")\n",
    "            # Play alarm sound\n",
    "            playsound(r\"C:\\Users\\mmmel\\Desktop\\AI_project\\fire-alarm-9677.mp3\")  # Replace with the actual path to your alarm sound file\n",
    "        else:\n",
    "            self.alarm_label.config(text=\"No Fire Detected\", fg=\"green\")\n",
    "\n",
    "    def update_gui(self):\n",
    "        self.root.update()\n",
    "\n",
    "    def __call__(self):\n",
    "        cap = cv2.VideoCapture(self.capture_index)\n",
    "        assert cap.isOpened()\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                start_time = time()\n",
    "                ret, frame = cap.read()\n",
    "                assert ret\n",
    "                results = self.predict(frame)\n",
    "                self.plot_bboxes(results)\n",
    "                end_time = time()\n",
    "                fps = 1/np.round(end_time - start_time, 2)\n",
    "                cv2.putText(frame, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "                # Commented out the display_image(frame) line\n",
    "                #self.display_image(frame)\n",
    "                self.update_gui()\n",
    "                sleep(0.01)  # Small delay to allow GUI to update\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"KeyboardInterrupt: Stopping the video stream and closing the GUI gracefully.\")\n",
    "        finally:\n",
    "            cap.release()\n",
    "            self.root.destroy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detector = ObjectDetection(capture_index=0)\n",
    "    detector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f837108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device:  cpu\n",
      "Model summary (fused): 168 layers, 11126745 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\n",
      "0: 480x800 1 smoke, 228.4ms\n",
      "Speed: 5.0ms preprocess, 228.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 smoke, 221.7ms\n",
      "Speed: 3.0ms preprocess, 221.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 1 smoke, 216.0ms\n",
      "Speed: 4.0ms preprocess, 216.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 2 smokes, 198.1ms\n",
      "Speed: 4.2ms preprocess, 198.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 1 smoke, 206.9ms\n",
      "Speed: 5.0ms preprocess, 206.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 2 smokes, 196.9ms\n",
      "Speed: 4.5ms preprocess, 196.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 1 smoke, 211.8ms\n",
      "Speed: 4.0ms preprocess, 211.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 1 smoke, 200.8ms\n",
      "Speed: 3.0ms preprocess, 200.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 1 smoke, 169.8ms\n",
      "Speed: 3.3ms preprocess, 169.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 1 smoke, 184.1ms\n",
      "Speed: 3.1ms preprocess, 184.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 smoke, 202.5ms\n",
      "Speed: 3.3ms preprocess, 202.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 202.5ms\n",
      "Speed: 3.0ms preprocess, 202.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 205.5ms\n",
      "Speed: 5.0ms preprocess, 205.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 212.2ms\n",
      "Speed: 4.5ms preprocess, 212.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 210.8ms\n",
      "Speed: 3.0ms preprocess, 210.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 194.1ms\n",
      "Speed: 4.3ms preprocess, 194.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 219.2ms\n",
      "Speed: 5.0ms preprocess, 219.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 smoke, 205.5ms\n",
      "Speed: 3.0ms preprocess, 205.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 205.5ms\n",
      "Speed: 3.0ms preprocess, 205.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 209.4ms\n",
      "Speed: 4.3ms preprocess, 209.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 213.7ms\n",
      "Speed: 4.1ms preprocess, 213.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 207.4ms\n",
      "Speed: 4.0ms preprocess, 207.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 210.3ms\n",
      "Speed: 3.5ms preprocess, 210.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 213.1ms\n",
      "Speed: 4.1ms preprocess, 213.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 210.4ms\n",
      "Speed: 3.4ms preprocess, 210.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 208.8ms\n",
      "Speed: 4.2ms preprocess, 208.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 216.7ms\n",
      "Speed: 3.0ms preprocess, 216.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 215.8ms\n",
      "Speed: 4.2ms preprocess, 215.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 206.2ms\n",
      "Speed: 4.1ms preprocess, 206.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 206.6ms\n",
      "Speed: 3.3ms preprocess, 206.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 209.3ms\n",
      "Speed: 5.0ms preprocess, 209.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 211.4ms\n",
      "Speed: 4.0ms preprocess, 211.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 225.9ms\n",
      "Speed: 4.0ms preprocess, 225.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 210.0ms\n",
      "Speed: 4.0ms preprocess, 210.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 218.6ms\n",
      "Speed: 4.0ms preprocess, 218.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 194.8ms\n",
      "Speed: 4.0ms preprocess, 194.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 225.2ms\n",
      "Speed: 3.2ms preprocess, 225.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 222.1ms\n",
      "Speed: 4.4ms preprocess, 222.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 216.0ms\n",
      "Speed: 3.0ms preprocess, 216.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 223.8ms\n",
      "Speed: 5.5ms preprocess, 223.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 226.4ms\n",
      "Speed: 4.0ms preprocess, 226.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 224.9ms\n",
      "Speed: 4.2ms preprocess, 224.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 223.0ms\n",
      "Speed: 4.0ms preprocess, 223.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 232.9ms\n",
      "Speed: 4.1ms preprocess, 232.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 2 defaults, 228.9ms\n",
      "Speed: 4.0ms preprocess, 228.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 229.1ms\n",
      "Speed: 4.0ms preprocess, 229.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 214.1ms\n",
      "Speed: 3.0ms preprocess, 214.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 2 defaults, 238.4ms\n",
      "Speed: 4.2ms preprocess, 238.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 228.2ms\n",
      "Speed: 4.2ms preprocess, 228.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 227.6ms\n",
      "Speed: 4.4ms preprocess, 227.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 225.8ms\n",
      "Speed: 5.2ms preprocess, 225.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 227.7ms\n",
      "Speed: 4.0ms preprocess, 227.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 2 defaults, 239.3ms\n",
      "Speed: 3.7ms preprocess, 239.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 2 defaults, 223.7ms\n",
      "Speed: 6.2ms preprocess, 223.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 1 smoke, 223.9ms\n",
      "Speed: 4.5ms preprocess, 223.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 2 defaults, 221.6ms\n",
      "Speed: 4.4ms preprocess, 221.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 230.1ms\n",
      "Speed: 4.0ms preprocess, 230.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 219.6ms\n",
      "Speed: 4.0ms preprocess, 219.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 226.1ms\n",
      "Speed: 3.5ms preprocess, 226.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 222.2ms\n",
      "Speed: 4.0ms preprocess, 222.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 smoke, 224.9ms\n",
      "Speed: 4.0ms preprocess, 224.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x800 1 smoke, 210.0ms\n",
      "Speed: 3.3ms preprocess, 210.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 219.9ms\n",
      "Speed: 4.4ms preprocess, 219.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 231.0ms\n",
      "Speed: 3.1ms preprocess, 231.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 2 defaults, 226.8ms\n",
      "Speed: 4.3ms preprocess, 226.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 2 defaults, 225.8ms\n",
      "Speed: 5.1ms preprocess, 225.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 2 defaults, 233.7ms\n",
      "Speed: 4.2ms preprocess, 233.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 221.2ms\n",
      "Speed: 4.3ms preprocess, 221.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 1 smoke, 226.1ms\n",
      "Speed: 3.2ms preprocess, 226.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 228.3ms\n",
      "Speed: 4.0ms preprocess, 228.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 2 smokes, 212.6ms\n",
      "Speed: 4.0ms preprocess, 212.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 1 smoke, 231.6ms\n",
      "Speed: 4.0ms preprocess, 231.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 2 smokes, 221.8ms\n",
      "Speed: 3.0ms preprocess, 221.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 smoke, 211.2ms\n",
      "Speed: 5.0ms preprocess, 211.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 smoke, 208.5ms\n",
      "Speed: 3.4ms preprocess, 208.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 2 smokes, 230.3ms\n",
      "Speed: 3.1ms preprocess, 230.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 smoke, 233.1ms\n",
      "Speed: 4.6ms preprocess, 233.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 smoke, 228.9ms\n",
      "Speed: 4.3ms preprocess, 228.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 smoke, 219.6ms\n",
      "Speed: 4.5ms preprocess, 219.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 225.0ms\n",
      "Speed: 3.0ms preprocess, 225.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 216.1ms\n",
      "Speed: 4.0ms preprocess, 216.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 222.0ms\n",
      "Speed: 5.0ms preprocess, 222.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 (no detections), 229.8ms\n",
      "Speed: 4.4ms preprocess, 229.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 smoke, 231.9ms\n",
      "Speed: 4.0ms preprocess, 231.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 smoke, 220.4ms\n",
      "Speed: 4.0ms preprocess, 220.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 2 smokes, 220.9ms\n",
      "Speed: 4.2ms preprocess, 220.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 237.1ms\n",
      "Speed: 4.6ms preprocess, 237.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 265.8ms\n",
      "Speed: 5.1ms preprocess, 265.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 269.5ms\n",
      "Speed: 4.4ms preprocess, 269.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 283.4ms\n",
      "Speed: 4.0ms preprocess, 283.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 251.4ms\n",
      "Speed: 6.0ms preprocess, 251.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 480x800 1 default, 361.3ms\n",
      "Speed: 7.1ms preprocess, 361.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 800)\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "can't invoke \"destroy\" command: application has been destroyed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16048\\712732419.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_bboxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m                 \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16048\\712732419.py\u001b[0m in \u001b[0;36mplot_bboxes\u001b[1;34m(self, results)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malarm_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"No Fire Detected\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"green\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mconfigure\u001b[1;34m(self, cnf, **kw)\u001b[0m\n\u001b[0;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m-> 1646\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_configure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'configure'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36m_configure\u001b[1;34m(self, cmd, cnf, kw)\u001b[0m\n\u001b[0;32m   1635\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getconfigure1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcnf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1636\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1637\u001b[0m     \u001b[1;31m# These used to be defined in Widget:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTclError\u001b[0m: invalid command name \".!label\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16048\\712732419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mObjectDetection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcapture_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[0mdetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16048\\712732419.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mdestroy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2310\u001b[0m         end the application of this Tcl interpreter.\"\"\"\n\u001b[0;32m   2311\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2312\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'destroy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2313\u001b[0m         \u001b[0mMisc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2314\u001b[0m         \u001b[1;32mglobal\u001b[0m \u001b[0m_default_root\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTclError\u001b[0m: can't invoke \"destroy\" command: application has been destroyed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from time import time, sleep\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "import tkinter as tk\n",
    "from playsound import playsound\n",
    "\n",
    "class ObjectDetection:\n",
    "\n",
    "    def __init__(self, capture_index):\n",
    "        self.capture_index = capture_index\n",
    "        self.device = 'cpu'\n",
    "        print(\"Using Device: \", self.device)\n",
    "        self.model = self.load_model()\n",
    "        self.CLASS_NAMES_DICT = self.model.model.names\n",
    "\n",
    "        # GUI setup\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"Fire Detection Alarm\")\n",
    "\n",
    "        # Alarm control variables\n",
    "        self.alarm_active = False\n",
    "        self.consecutive_frames_with_fire = 0\n",
    "        self.frames_to_trigger_alarm = 5  # Adjust as needed\n",
    "\n",
    "        # GUI components\n",
    "        self.alarm_label = tk.Label(self.root, text=\"No Fire Detected\", font=(\"Helvetica\", 16))\n",
    "        self.alarm_label.pack()\n",
    "\n",
    "        self.turn_off_alarm_button = tk.Button(self.root, text=\"Turn Off Alarm\", command=self.turn_off_alarm)\n",
    "        self.turn_off_alarm_button.pack()\n",
    "\n",
    "    def load_model(self):\n",
    "        model = YOLO(r\"C:\\Users\\mmmel\\Desktop\\AI_project\\best.pt\")\n",
    "        model.fuse()\n",
    "        return model\n",
    "\n",
    "    def predict(self, frame):\n",
    "        results = self.model(frame)\n",
    "        # print(results)\n",
    "        return results\n",
    "\n",
    "    def plot_bboxes(self, results):\n",
    "        fire_detected = False\n",
    "\n",
    "        for result in results[0]:\n",
    "            class_id = result.boxes.cls.cpu().numpy().astype(int)\n",
    "            if class_id == 0:\n",
    "                fire_detected = True\n",
    "\n",
    "        # Update GUI label\n",
    "        if fire_detected:\n",
    "            self.consecutive_frames_with_fire += 1\n",
    "            if self.consecutive_frames_with_fire >= self.frames_to_trigger_alarm and not self.alarm_active:\n",
    "                self.trigger_alarm()\n",
    "        else:\n",
    "            self.consecutive_frames_with_fire = 0\n",
    "\n",
    "        if self.alarm_active:\n",
    "            self.alarm_label.config(text=\"Fire Detected! Alarm Active\", fg=\"red\")\n",
    "        else:\n",
    "            self.alarm_label.config(text=\"No Fire Detected\", fg=\"green\")\n",
    "\n",
    "    def trigger_alarm(self):\n",
    "        print(\"Fire detected in at least 5 consecutive frames. Triggering alarm!\")\n",
    "        self.alarm_active = True\n",
    "        # Play alarm sound\n",
    "        playsound(r\"C:\\Users\\mmmel\\Desktop\\AI_project\\fire-alarm-9677.mp3\")  # Replace with the actual path to your alarm sound file\n",
    "\n",
    "    def turn_off_alarm(self):\n",
    "        print(\"Turning off alarm manually.\")\n",
    "        self.alarm_active = False\n",
    "\n",
    "    def update_gui(self):\n",
    "        self.root.update()\n",
    "\n",
    "    def __call__(self):\n",
    "        cap = cv2.VideoCapture(self.capture_index)\n",
    "        assert cap.isOpened()\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                start_time = time()\n",
    "                ret, frame = cap.read()\n",
    "                assert ret\n",
    "                results = self.predict(frame)\n",
    "                self.plot_bboxes(results)\n",
    "                end_time = time()\n",
    "                fps = 1 / np.round(end_time - start_time, 2)\n",
    "                cv2.putText(frame, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "                # Display the frame using IPython.display (commented out)\n",
    "                # self.display_image(frame)\n",
    "                self.update_gui()\n",
    "                sleep(0.01)  # Small delay to allow GUI to update\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"KeyboardInterrupt: Stopping the video stream and closing the GUI gracefully.\")\n",
    "        finally:\n",
    "            cap.release()\n",
    "            self.root.destroy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detector = ObjectDetection(capture_index=0)\n",
    "    detector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9751d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
